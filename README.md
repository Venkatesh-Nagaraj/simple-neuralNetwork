# simple-neuralNetwork
Our NN will have a simple two-layer architecture. Input layer  ğ‘[0]
  will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer  ğ‘[1]
  will have 10 units with ReLU activation, and finally our output layer  ğ‘[2]
  will have 10 units corresponding to the ten digit classes with softmax activation.

Forward propagation
                       ğ‘[1]=ğ‘Š[1]ğ‘‹+ğ‘[1]
                       ğ´[1]=ğ‘”ReLU(ğ‘[1]))
                       ğ‘[2]=ğ‘Š[2]ğ´[1]+ğ‘[2]
                       ğ´[2]=ğ‘”softmax(ğ‘[2])
 
Backward propagation
                        ğ‘‘ğ‘[2]=ğ´[2]âˆ’ğ‘Œ
                        ğ‘‘ğ‘Š[2]=1ğ‘šğ‘‘ğ‘[2]ğ´[1]ğ‘‡
                        ğ‘‘ğµ[2]=1ğ‘šÎ£ğ‘‘ğ‘[2]
                        ğ‘‘ğ‘[1]=ğ‘Š[2]ğ‘‡ğ‘‘ğ‘[2].âˆ—ğ‘”[1]â€²(ğ‘§[1])
                        ğ‘‘ğ‘Š[1]=1ğ‘šğ‘‘ğ‘[1]ğ´[0]ğ‘‡
                        ğ‘‘ğµ[1]=1ğ‘šÎ£ğ‘‘ğ‘[1]
 
Parameter updates
                    ğ‘Š[2]:=ğ‘Š[2]âˆ’ğ›¼ğ‘‘ğ‘Š[2] 
                    ğ‘[2]:=ğ‘[2]âˆ’ğ›¼ğ‘‘ğ‘[2]
                    ğ‘Š[1]:=ğ‘Š[1]âˆ’ğ›¼ğ‘‘ğ‘Š[1]
                    ğ‘[1]:=ğ‘[1]âˆ’ğ›¼ğ‘‘ğ‘[1]
